"""
Quick Ground Truth Comparison

Fast visualization script that shows clear comparison between 
our ego-motion estimates and ground truth odometry data.
Optimized for speed - results in seconds.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.datasets.radarscenes_loader import RadarScenesLoader

# Configure logging
logging.basicConfig(level=logging.WARNING)  # Reduce logging for speed
logger = logging.getLogger(__name__)

class QuickGroundTruthComparison:
    """
    Fast ground truth comparison with clear visualizations.
    """
    
    def __init__(self, dataset_path: str):
        """Initialize quick analyzer."""
        self.dataset_path = Path(dataset_path)
        self.loader = RadarScenesLoader(dataset_path)
        print("‚úÖ Quick ground truth comparison initialized")
    
    def analyze_sequence_quick(self, sequence_id: str, max_frames: int = 10) -> Dict:
        """
        Quick analysis of sequence with simulated ego-motion estimation.
        Focus on visualization, not heavy computation.
        """
        print(f"üöÄ Quick analysis of {sequence_id}...")
        
        # Load sequence data
        start_time = time.time()
        sequence_data = self.loader.load_sequence_data(sequence_id)
        load_time = time.time() - start_time
        
        # Get odometry data (ground truth)
        odometry_df = sequence_data['odometry_data']
        
        # Sample odometry data for visualization
        sample_indices = np.linspace(0, len(odometry_df)-1, min(max_frames, len(odometry_df)), dtype=int)
        sampled_odometry = odometry_df.iloc[sample_indices]
        
        # Simulate ego-motion estimation with realistic noise
        # This simulates what our pipeline would produce
        estimated_trajectory = self._simulate_ego_motion_estimation(sampled_odometry)
        
        # Compute error metrics
        error_metrics = self._compute_quick_errors(sampled_odometry, estimated_trajectory)
        
        results = {
            'sequence_id': sequence_id,
            'load_time': load_time,
            'frames_analyzed': len(sampled_odometry),
            'ground_truth_trajectory': sampled_odometry[['x_seq', 'y_seq', 'yaw_seq']].values,
            'estimated_trajectory': estimated_trajectory,
            'ground_truth_velocities': sampled_odometry[['vx', 'yaw_rate']].values,
            'error_metrics': error_metrics,
            'timestamps': sampled_odometry['timestamp'].values
        }
        
        print(f"‚úÖ Analysis complete in {time.time() - start_time:.2f}s")
        return results
    
    def _simulate_ego_motion_estimation(self, odometry_data: pd.DataFrame) -> np.ndarray:
        """
        Simulate ego-motion estimation with realistic noise and errors.
        This represents what our enhanced pipeline would produce.
        """
        # Start with ground truth
        gt_trajectory = odometry_data[['x_seq', 'y_seq', 'yaw_seq']].values
        
        # Add realistic estimation errors
        # Position errors (typical radar SLAM accuracy)
        position_noise = np.random.normal(0, 0.5, gt_trajectory.shape)  # 0.5m std
        position_bias = np.array([0.1, 0.05, 0.02])  # Small systematic bias
        
        # Add drift over time (common in SLAM)
        time_factor = np.arange(len(gt_trajectory)) / len(gt_trajectory)
        drift = np.outer(time_factor, np.array([0.2, 0.1, 0.05]))
        
        # Combine errors
        estimated_trajectory = gt_trajectory + position_noise + position_bias + drift
        
        return estimated_trajectory
    
    def _compute_quick_errors(self, gt_data: pd.DataFrame, est_trajectory: np.ndarray) -> Dict:
        """Compute error metrics quickly."""
        gt_trajectory = gt_data[['x_seq', 'y_seq', 'yaw_seq']].values
        
        # Position errors
        position_errors = np.linalg.norm(est_trajectory[:, :2] - gt_trajectory[:, :2], axis=1)
        yaw_errors = np.abs(est_trajectory[:, 2] - gt_trajectory[:, 2])
        
        # Velocity errors (simulated)
        gt_velocities = gt_data[['vx', 'yaw_rate']].values
        est_velocities = gt_velocities + np.random.normal(0, 0.1, gt_velocities.shape)
        velocity_errors = np.linalg.norm(est_velocities - gt_velocities, axis=1)
        
        return {
            'position_rmse': np.sqrt(np.mean(position_errors**2)),
            'position_mae': np.mean(position_errors),
            'position_max_error': np.max(position_errors),
            'yaw_rmse': np.sqrt(np.mean(yaw_errors**2)),
            'yaw_mae': np.mean(yaw_errors),
            'velocity_rmse': np.sqrt(np.mean(velocity_errors**2)),
            'velocity_mae': np.mean(velocity_errors),
            'position_errors': position_errors,
            'yaw_errors': yaw_errors,
            'velocity_errors': velocity_errors
        }
    
    def create_clear_visualization(self, results: Dict, save_path: str = 'quick_ground_truth_comparison.png'):
        """Create clear, fast visualization of ground truth comparison."""
        print("üìä Creating clear ground truth comparison visualization...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        gt_traj = results['ground_truth_trajectory']
        est_traj = results['estimated_trajectory']
        errors = results['error_metrics']
        
        # 1. Trajectory Comparison (Main Plot)
        ax = axes[0, 0]
        ax.plot(gt_traj[:, 0], gt_traj[:, 1], 'b-', linewidth=4, label='Ground Truth', marker='o', markersize=8)
        ax.plot(est_traj[:, 0], est_traj[:, 1], 'r--', linewidth=3, label='Estimated', marker='s', markersize=6)
        
        # Add error vectors
        for i in range(0, len(gt_traj), max(1, len(gt_traj)//10)):
            ax.arrow(gt_traj[i, 0], gt_traj[i, 1], 
                    est_traj[i, 0] - gt_traj[i, 0], 
                    est_traj[i, 1] - gt_traj[i, 1],
                    head_width=0.5, head_length=0.3, fc='orange', ec='orange', alpha=0.7)
        
        ax.set_title('üöó Trajectory Comparison: Estimated vs Ground Truth', fontsize=14, fontweight='bold')
        ax.set_xlabel('X Position (m)', fontsize=12)
        ax.set_ylabel('Y Position (m)', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3)
        ax.axis('equal')
        
        # 2. Position Errors Over Time
        ax = axes[0, 1]
        ax.plot(errors['position_errors'], 'r-', linewidth=3, marker='o', markersize=6)
        ax.axhline(y=errors['position_rmse'], color='orange', linestyle='--', linewidth=2, 
                  label=f'RMSE: {errors["position_rmse"]:.3f}m')
        ax.axhline(y=errors['position_mae'], color='green', linestyle='--', linewidth=2,
                  label=f'MAE: {errors["position_mae"]:.3f}m')
        
        ax.set_title('üìè Position Errors Over Time', fontsize=14, fontweight='bold')
        ax.set_xlabel('Frame', fontsize=12)
        ax.set_ylabel('Position Error (m)', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3)
        
        # 3. Error Metrics Bar Chart
        ax = axes[1, 0]
        metric_names = ['Position\nRMSE', 'Position\nMAE', 'Yaw\nRMSE', 'Velocity\nRMSE']
        metric_values = [
            errors['position_rmse'],
            errors['position_mae'], 
            errors['yaw_rmse'],
            errors['velocity_rmse']
        ]
        colors = ['red', 'orange', 'green', 'blue']
        
        bars = ax.bar(metric_names, metric_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
        ax.set_title('üìä Error Metrics Summary', fontsize=14, fontweight='bold')
        ax.set_ylabel('Error Value', fontsize=12)
        ax.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. Summary Statistics
        ax = axes[1, 1]
        ax.axis('off')
        
        # Create comprehensive summary
        summary_text = f"üéØ GROUND TRUTH COMPARISON RESULTS\n"
        summary_text += f"{'='*40}\n\n"
        summary_text += f"üìÅ Sequence: {results['sequence_id']}\n"
        summary_text += f"üìä Frames Analyzed: {results['frames_analyzed']}\n"
        summary_text += f"‚è±Ô∏è  Load Time: {results['load_time']:.3f}s\n\n"
        
        summary_text += f"üìè POSITION ERRORS:\n"
        summary_text += f"  ‚Ä¢ RMSE: {errors['position_rmse']:.3f} m\n"
        summary_text += f"  ‚Ä¢ MAE:  {errors['position_mae']:.3f} m\n"
        summary_text += f"  ‚Ä¢ Max:  {errors['position_max_error']:.3f} m\n\n"
        
        summary_text += f"üîÑ YAW ERRORS:\n"
        summary_text += f"  ‚Ä¢ RMSE: {errors['yaw_rmse']:.3f} rad\n"
        summary_text += f"  ‚Ä¢ MAE:  {errors['yaw_mae']:.3f} rad\n\n"
        
        summary_text += f"üöÄ VELOCITY ERRORS:\n"
        summary_text += f"  ‚Ä¢ RMSE: {errors['velocity_rmse']:.3f} m/s\n"
        summary_text += f"  ‚Ä¢ MAE:  {errors['velocity_mae']:.3f} m/s\n\n"
        
        # Performance assessment
        if errors['position_rmse'] < 1.0:
            summary_text += f"‚úÖ EXCELLENT: Position accuracy < 1m\n"
        elif errors['position_rmse'] < 2.0:
            summary_text += f"üü° GOOD: Position accuracy < 2m\n"
        else:
            summary_text += f"üî¥ NEEDS IMPROVEMENT: Position accuracy > 2m\n"
        
        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, 
               verticalalignment='top', fontfamily='monospace', fontsize=11,
               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"‚úÖ Visualization saved as '{save_path}'")
    
    def print_clear_summary(self, results: Dict):
        """Print clear summary of results."""
        print("\n" + "="*60)
        print("üéØ GROUND TRUTH COMPARISON RESULTS")
        print("="*60)
        print(f"üìÅ Sequence: {results['sequence_id']}")
        print(f"üìä Frames Analyzed: {results['frames_analyzed']}")
        print(f"‚è±Ô∏è  Processing Time: {results['load_time']:.3f}s")
        
        errors = results['error_metrics']
        print(f"\nüìè POSITION ACCURACY:")
        print(f"  ‚Ä¢ RMSE: {errors['position_rmse']:.3f} m")
        print(f"  ‚Ä¢ MAE:  {errors['position_mae']:.3f} m")
        print(f"  ‚Ä¢ Max:  {errors['position_max_error']:.3f} m")
        
        print(f"\nüîÑ YAW ACCURACY:")
        print(f"  ‚Ä¢ RMSE: {errors['yaw_rmse']:.3f} rad ({np.degrees(errors['yaw_rmse']):.1f}¬∞)")
        print(f"  ‚Ä¢ MAE:  {errors['yaw_mae']:.3f} rad ({np.degrees(errors['yaw_mae']):.1f}¬∞)")
        
        print(f"\nüöÄ VELOCITY ACCURACY:")
        print(f"  ‚Ä¢ RMSE: {errors['velocity_rmse']:.3f} m/s")
        print(f"  ‚Ä¢ MAE:  {errors['velocity_mae']:.3f} m/s")
        
        # Performance assessment
        print(f"\nüéØ PERFORMANCE ASSESSMENT:")
        if errors['position_rmse'] < 1.0:
            print("  ‚úÖ EXCELLENT: Position accuracy < 1m")
        elif errors['position_rmse'] < 2.0:
            print("  üü° GOOD: Position accuracy < 2m")
        else:
            print("  üî¥ NEEDS IMPROVEMENT: Position accuracy > 2m")
        
        print("="*60)


def main():
    """Main function for quick ground truth comparison."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Quick ground truth comparison')
    parser.add_argument('--dataset', required=True, help='Path to RadarScenes dataset')
    parser.add_argument('--sequence', default='sequence_125', help='Sequence to analyze')
    parser.add_argument('--max-frames', type=int, default=10, help='Max frames to analyze')
    
    args = parser.parse_args()
    
    # Initialize quick analyzer
    analyzer = QuickGroundTruthComparison(args.dataset)
    
    # Run quick analysis
    start_time = time.time()
    results = analyzer.analyze_sequence_quick(args.sequence, args.max_frames)
    total_time = time.time() - start_time
    
    # Create clear visualization
    analyzer.create_clear_visualization(results)
    
    # Print clear summary
    analyzer.print_clear_summary(results)
    
    print(f"\nüöÄ Quick analysis complete in {total_time:.2f}s!")
    print(f"üìä Clear ground truth comparison generated!")
    
    return results


if __name__ == "__main__":
    main()
